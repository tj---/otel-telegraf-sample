receivers:
 # Influx receiver (http) - Currently in Beta
 influxdb:
   # https://pkg.go.dev/github.com/open-telemetry/opentelemetry-collector-contrib/receiver/influxdbreceiver#section-readme
   # Apparently, only HTTP service endpoint for the line protocol receiver 
   endpoint: 0.0.0.0:19000

 # Carbon / Graphite receiver - Currently in Beta
 # All the details are present here:
 # https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/receiver/carbonreceiver
 carbon:
   endpoint: 0.0.0.0:2003
   transport: tcp
   tcp_idle_timeout: 10s
   parser:
     type: regex
     config:
       # Matches the first rule
       rules:
         # <key_X> makes labels while <name_Y> makes metric names
         # Cluster.c891.node74.Diamond-Telegraf.temperature.max
         - regexp: "^(?P<key_type>[^.]+)\\.(?P<key_custer_id>[^.]+)\\.(?P<key_node>[^.]+)\\.(?P<name_tkn3>[^.]+)\\.(?P<key_tkn4>[^.]+)\\.(?P<name_tkn5>[^.]+)$"
           name_prefix: ""
           type: cumulative
         # Cluster.c891.node74.Diamond-Telegraf.foo.bar.weather.xyz
         - regexp: "^(?P<key_type>[^.]+)\\.(?P<key_custer_id>[^.]+)\\.(?P<key_node>[^.]+)\\.(?P<key_tkn3>[^.]+)\\.(?P<key_tkn4>[^.]+)\\.(?P<key_tkn5>[^.]+)\\.(?P<name_measurement1>[^.]+)\\.(?P<name_measurement2>[^.]+)$"
           name_prefix: ""
           type: cumulative
       name_separator: "_"

 # OTLP receiver
 otlp:
   protocols:
     http:
       endpoint: 0.0.0.0:4318
     grpc:
       endpoint: 0.0.0.0:4317

processors:
 # https://github.com/open-telemetry/opentelemetry-collector/blob/main/processor/memorylimiterprocessor/README.md
 memory_limiter:
   check_interval: 1s
   limit_percentage: 50
   spike_limit_percentage: 30
 batch:
   # https://github.com/open-telemetry/opentelemetry-collector/blob/main/processor/batchprocessor/README.md
   # The batch processor accepts spans, metrics, or logs and places them into batches.
   # Batching helps better compress the data and reduce the number of outgoing connections required to transmit the data.
   # This processor supports both size and time based batching.
   timeout: 2s
 resource:
   attributes:
     # Dummy processor - Just adds a key/value pair to each metric flowing through
     - key: test.key
       value: "test-value"
       action: insert

exporters:
 # Multiple Exporters of the same "type" are possible, they have to be named <type>/<unique-name>
 file/file-A:
    path: /dev/stdout
 file/file-B:
    path: /dev/stdout
 logging:
   loglevel: debug
 # This enables a scraping endpoint for Prometheus
 prometheus:
   endpoint: 0.0.0.0:8889
   namespace: "cdm"
   metric_expiration: 5s

extensions:
 health_check:
 pprof:
   endpoint: :1888
 zpages:
   endpoint: :55679

service:
 extensions: [pprof, zpages, health_check]
 pipelines:
   metrics:
     receivers: [influxdb, otlp, carbon]
     processors: [batch, resource]
     exporters: [file/file-A, file/file-B, logging, prometheus]
